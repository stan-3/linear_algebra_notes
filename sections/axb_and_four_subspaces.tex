\documentclass[../main.tex]{subfiles}

\begin{document}

\begin{definition}[Matrix multiplication]
	If $\vect{A}$ is a matrix of size $m\times n$ and $\vect{B}$ is a matrix of size $n\times p$, then
	\begin{align*}
		\vect{A}\vect{B} & =
		\begin{bmatrix}
			a_{11} & a_{12} & \cdots & a_{1n} \\
			a_{21} & a_{22} & \cdots & a_{2n} \\
			\vdots & \vdots & \ddots & \vdots \\
			a_{m1} & a_{m2} & \cdots & a_{mn}
		\end{bmatrix}
		\begin{bmatrix}
			b_{11} & b_{12} & \cdots & b_{1p} \\
			b_{21} & b_{22} & \cdots & b_{2p} \\
			\vdots & \vdots & \ddots & \vdots \\
			b_{n1} & b_{n2} & \cdots & b_{np}
		\end{bmatrix}                                                                                                      \\
		                 & =\begin{bmatrix}
			                    a_{11}b_{11}+\cdots +a_{1n}b_{n1} & a_{11}b_{12}+\cdots +a_{1n}b_{n2} & \cdots & a_{11}b_{1p}+\cdots +a_{1n}b_{np} \\
			                    a_{21}b_{11}+\cdots +a_{2n}b_{n1} & a_{21}b_{12}+\cdots +a_{2n}b_{n2} & \cdots & a_{21}b_{1p}+\cdots +a_{2n}b_{np} \\
			                    \vdots                            & \vdots                            & \ddots & \vdots                            \\
			                    a_{m1}b_{11}+\cdots +a_{mn}b_{n1} & a_{m1}b_{12}+\cdots +a_{mn}b_{n2} & \cdots & a_{m1}b_{1p}+\cdots +a_{mn}b_{np}
		                    \end{bmatrix}
	\end{align*}
	This is like taking the dot product of the rows of $\vect{A}$ with the
	columns of $\vect{B}$. Notice that the result of $\vect{A}\vect{B}$
	is a matrix of size $m\times p$. This definition is extended to matrix-vector
	multiplication by treating a $n$-dimensional vector as a matrix of size $n\times 1$.

	In the case of matrix-vector multiplication, an equivalent
	and commonly more useful computation is taking the linear combination of the
	column vectors of $\vect{A}$ where the scalars are the corresponding
	components in $\vect{x}$; if $\vect{A}=\begin{bmatrix}\vect{v_1}&\vect{v_2}&\cdots&\vect{v_n}\end{bmatrix}$,
	then $\vect{A}\vect{x}=x_1\vect{v_1}+x_2\vect{v_2}+\cdots +x_n\vect{v_n}$.
\end{definition}

\begin{theorem}[Associativity of matrix multiplication]
	If $\vect{A} \in F^{m\times n}$, $\vect{B} \in F^{n\times p}$, and $\vect{C} \in F^{p\times q}$,
	then $(\vect{A}\vect{B})\vect{C}=\vect{A}(\vect{B}\vect{C})$.
\end{theorem}

\begin{proof}
	Note that $(\vect{A}\vect{B})_{m\times p}\vect{C}_{p\times q}$ is a valid product resulting
	in an $m\times q$  matrix, and $\vect{A}_{m\times n}(\vect{B}\vect{C})_{n\times q}$ is similarly
	well-defined and also results in an $m\times q$ matrix. To prove that $(\vect{A}\vect{B})\vect{C}=\vect{A}(\vect{B}\vect{C})$,
	we just have to show that every entry is equivalent: $((\vect{A}\vect{B})\vect{C})_{ij}=(\vect{A}(\vect{B}\vect{C}))_{ij}$ for
	$1\leq i\leq m$ and $1\leq j\leq q$.
	\begin{align*}
		LHS & =((\vect{A}\vect{B})_{m\times p}\vect{C}_{p\times q})_{ij}           \\
		    & =\sum_{k=1}^p (\vect{A}\vect{B})_{ik}\vect{C}_{kj}
		\intertext{This is derived from the definition of matrix multiplication; a resulting entry at the
			$i$th row and $j$th column is the ``dot product" between the $i$th row of the first matrix, $\vect{A}\vect{B}$,
			and the $j$th column of the second matrix, $\vect{C}$. We will similarly express $(\vect{A}_{m\times n}\vect{B}_{n\times p})_{ik}$ as a sum.}
		    & =\sum_{k=1}^p (\sum_{l=1}^n \vect{A}_{il}\vect{B}_{lk})\vect{C}_{kj}
		\intertext{By the distributive property,}
		    & =\sum_{k=1}^p \sum_{l=1}^n \vect{A}_{il}\vect{B}_{lk}\vect{C}_{kj}
	\end{align*}
	We'll do the same for the right-hand side.
	\begin{align*}
		RHS & =(\vect{A}_{m\times n}(\vect{B}\vect{C})_{n\times q})_{ij}                 \\
		    & =\sum_{l=1}^n \vect{A}_{il}(\vect{B}_{n\times p}\vect{C}_{p\times q})_{lj} \\
		    & =\sum_{l=1}^n \vect{A}_{il}(\sum_{k=1}^p \vect{B}_{lk}\vect{C}_{kj})       \\
		    & =\sum_{l=1}^n \sum_{k=1}^p \vect{A}_{il}\vect{B}_{lk}\vect{C}_{kj}
		\intertext{Since fields are commutative, we can swap the summations.}
		    & =\sum_{k=1}^p \sum_{l=1}^n \vect{A}_{il}\vect{B}_{lk}\vect{C}_{kj}
	\end{align*}
	$LHS=RHS$.
\end{proof}

\subsection{Elimination with matrices}

We can set up a \textit{coefficient matrix} to solve a linear system of equations.
Then, in the process called \textit{Gaussian elimination}, we manipulate the
rows of the matrix by combining different multiples of each row. The
desired result is a matrix in \textit{row echelon form}, which contains
\textit{pivot entries} in each column. A pivot entry is a nonzero entry
which sits below and to the right of the previous pivot entry.
Every entry under the pivot entry must be zero. The first entry
of a matrix in row echelon form should be a pivot entry.

\begin{example}
	Solve the following linear system:
	\[\begin{array}{rrrrrrr}
			x  & + & 2y & + & z & = & 2  \\
			3x & + & 8y & + & z & = & 12 \\
			   &   & 4y & + & z & = & 2
		\end{array}\]
\end{example}

\begin{solution} \label{sol:linear_sys_1}
	Let $\vect{A}$ be the coefficient matrix. We will use Gaussian elimination to
	put $\vect{A}$ in row echelon form.
	\[\vect{A}=\begin{bmatrix}1&2&1\\3&8&1\\0&4&1\end{bmatrix}
		\xrightarrow{\vect{r_2}-3\vect{r_1}} \begin{bmatrix}1&2&1\\0&2&-2\\0&4&1\end{bmatrix}
		\xrightarrow{\vect{r_3}-2\vect{r_2}} \begin{bmatrix}\underline{1}&2&1\\0&\underline{2}&-2\\0&0&\underline{5}\end{bmatrix}=\vect{U} \]
	The underlined entries in the $\vect{U}=\operatorname{ref}(\vect{A})$ are the pivot entries.
	While this example worked out well (we were able to find $\operatorname{ref}(\vect{A})$),
	the process is not always straightforward. For example, if the first original
	entry of $\vect{A}$ was $0$, we would have to swap the first row with a suitable row beneath.
	Likewise, if we came across a zero in a pivot position in a later step, we could again try to exchange
	the row with a suitable row beneath. Still, there are cases where a pivot entry cannot be found.

	Let's repeat the same process but with an augmented matrix this time.
	An \textit{augmented matrix} adds the column containing the solutions to
	the coefficient matrix.
	\[\left[\begin{array}{rrr|r}
				1 & 2 & 1 & 2  \\
				3 & 8 & 1 & 12 \\
				0 & 4 & 1 & 2
			\end{array}\right]\to
		\left[\begin{array}{rrr|r}
				1 & 2 & 1  & 2 \\
				0 & 2 & -2 & 6 \\
				0 & 4 & 1  & 2
			\end{array}\right]\to
		\left[\begin{array}{rrr|r}
				1 & 2 & 1  & 2   \\
				0 & 2 & -2 & 6   \\
				0 & 0 & 5  & -10
			\end{array}\right]\]
	We can rewrite the system of equations:
	\begin{align*}
		x+2y+z & =2 & 2y-2z & =6 & 5z & =-10 \\
	\end{align*}
	\[z=-2\]
	\[2y-2(-2)=6\implies y=1\]
	\[x+2(1)+(-2)=2\implies x=2\]
	\flushright
	\qedsymbol
\end{solution}

Let's try to solve the same system using matrix multiplication to show our steps.
First of all, suppose $\vect{x} \in F^n$ and $\vect{B}=\begin{bmatrix}\vect{r_1}\\\vdots\\\vect{r_n}\end{bmatrix}$
where $\vect{r_i} \in F^m$ is a row vector. Then,
\begin{align*}
	\vect{x}^T\vect{B} & =\begin{bmatrix}x_1&\cdots&x_n\end{bmatrix}\begin{bmatrix}\vect{r_1}\\\vdots\\\vect{r_n}\end{bmatrix} \\
	                   & =x_1\vect{r_1}+x_2\vect{r_2}+\cdots +x_n\vect{r_n}
\end{align*}
Note that the result is a row vector of size $1\times m$. This row-matrix
multiplication, which yields a linear combination of the rows of a matrix,
is analagous to matrix-vector multiplication which yields a linear
combination of the columns of a matrix.

Suppose now that $\vect{r_i}$ is a $1\times 3$ row vector. Consider
this operation:
\[\begin{bmatrix}\vect{r_1}\\\vect{r_2}\\\vect{r_3}\end{bmatrix}
	\to[\vect{r_1}].\]
Clearly, we can represent the operation using this row-matrix product:
\[\begin{bmatrix}1&0&0\end{bmatrix}\begin{bmatrix}\vect{r_1}\\\vect{r_2}\\\vect{r_3}\end{bmatrix}=[\vect{r_1}].\]
Now consider this operation:
\[\begin{bmatrix}\vect{r_1}\\\vect{r_2}\\\vect{r_3}\end{bmatrix}\to\begin{bmatrix}\vect{r_1}\\\vect{r_2}\\\vect{r_3}\end{bmatrix}.\]
Firstly, the matrix we select should have a size of $3\times 3$. The first row of the result
is $\vect{r_1}$, just like in the previous example. So, it makes sense to make the first row
of the matrix $\begin{bmatrix}1&0&0\end{bmatrix}$, just like before. Similarly, to get
$\vect{r_2}$ in the second row of the result, we can make the second row
of the matrix $\begin{bmatrix}0&1&0\end{bmatrix}$. Then, we do the same for
the third row. The matrix multiplication looks like this:
\[\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}\begin{bmatrix}\vect{r_1}\\\vect{r_2}\\\vect{r_3}\end{bmatrix}=\begin{bmatrix}\vect{r_1}\\\vect{r_2}\\\vect{r_3}\end{bmatrix}\]
The matrix we just created is a special matrix called the \textit{identity matrix}. Specifically,
this is a $3\times 3$ identity matrix, typically labelled $I_3$.

\begin{definition}[Identity matrix]
	An $n\times n$ identity matrix,
	denoted $I_n$, is a square matrix with ones along its diagonal and zeros everywhere else. An identity matrix
	has the property that $I_m\vect{C}=\vect{C}$ where $\vect{C}$ is a $m\times n$ matrix.
\end{definition}

\begin{theorem}
	Suppose $\vect{E} \in F^{m\times m}$ and $\vect{A} \in F^{m\times n}$. Let $\vect{r_i} \in F^{1\times n}$
	be the row vector representing the $i$th row of $\vect{A}$ for $i=1,\ldots,m$.
	Then, an equivalent ``row-based" matrix multiplication can be performed like this:
	\[\vect{E}\vect{A}=\begin{bmatrix}e_{11} & \cdots & e_{1m} \\
               \vdots & \ddots & \vdots \\
               e_{m1} & \cdots & e_{mm}\end{bmatrix}
		\begin{bmatrix}\vect{r_1}\\\vdots\\\vect{r_m}\end{bmatrix}
		=\begin{bmatrix}e_{11}\vect{r_1}+\cdots +e_{1m}\vect{r_m}\\\vdots\\e_{m1}\vect{r_1}+\cdots +e_{mm}\vect{r_m}\end{bmatrix}\]
\end{theorem}

\begin{proof}
	Since $\vect{r_i}$ is the row vector representing the $i$th row of
	$\vect{A}$, it has the form\\
	$\vect{r_i}=\begin{bmatrix}a_{i1}&a_{i2}&\cdots&a_{in}\end{bmatrix}$.
	\begin{align*}
		\vect{E}\vect{A} & =\begin{bmatrix}
			                    e_{11}a_{11}+\cdots+e_{1m}a_{m1} & \cdots & e_{11}a_{1n}+\cdots +e_{1m}a_{mn} \\
			                    e_{21}a_{11}+\cdots+e_{2m}a_{m1} & \cdots & e_{21}a_{1n}+\cdots +e_{2m}a_{mn} \\
			                    \vdots                           & \ddots & \vdots                            \\
			                    e_{m1}a_{11}+\cdots+e_{mm}a_{m1} & \cdots & e_{m1}a_{1n}+\cdots +e_{mm}a_{mn}
		                    \end{bmatrix} \\
		                 & =e_{11}\begin{bmatrix}
			                          a_{11} & \cdots & a_{1n} \\
			                          0      & \cdots & 0      \\
			                          \vdots & \ddots & \vdots \\
			                          0      & \cdots & 0
		                          \end{bmatrix}+\cdots+
		e_{1m}\begin{bmatrix}
			      a_{m1} & \cdots & a_{mn} \\
			      0      & \cdots & 0      \\
			      \vdots & \ddots & \vdots \\
			      0      & \cdots & 0
		      \end{bmatrix}+
		e_{21}\begin{bmatrix}
			      0      & \cdots & 0      \\
			      a_{11} & \cdots & a_{1n} \\
			      \vdots & \ddots & \vdots \\
			      0      & \cdots & 0
		      \end{bmatrix}                                                                    \\
		                 & +\cdots+e_{2m}\begin{bmatrix}
			                                 0      & \cdots & 0      \\
			                                 a_{m1} & \cdots & a_{mn} \\
			                                 \vdots & \ddots & \vdots \\
			                                 0      & \cdots & 0
		                                 \end{bmatrix}+\cdots+
		e_{m1}\begin{bmatrix}
			      0      & \cdots & 0      \\
			      \vdots & \ddots & \vdots \\
			      0      & \cdots & 0      \\
			      a_{11} & \cdots & a_{1n}
		      \end{bmatrix}+\cdots+
		e_{mm}\begin{bmatrix}
			      0      & \cdots & 0      \\
			      \vdots & \ddots & \vdots \\
			      0      & \cdots & 0      \\
			      a_{m1} & \cdots & a_{mn}
		      \end{bmatrix}                                                                    \\
		                 & =\begin{bmatrix}
			                           & e_{11}\vect{r_1} &        \\
			                    0      & \cdots           & 0      \\
			                    \vdots & \ddots           & \vdots \\
			                    0      & \cdots           & 0
		                    \end{bmatrix}+\cdots+
		\begin{bmatrix}
			       & e_{1m}\vect{r_m} &        \\
			0      & \cdots           & 0      \\
			\vdots & \ddots           & \vdots \\
			0      & \cdots           & 0
		\end{bmatrix}+
		\begin{bmatrix}
			0      & \cdots           & 0      \\
			       & e_{21}\vect{r_1} &        \\
			\vdots & \ddots           & \vdots \\
			0      & \cdots           & 0
		\end{bmatrix}+\cdots+
		\begin{bmatrix}
			0      & \cdots           & 0      \\
			       & e_{2m}\vect{r_m} &        \\
			\vdots & \ddots           & \vdots \\
			0      & \cdots           & 0
		\end{bmatrix}                                                                \\
		                 & +\cdots+\begin{bmatrix}
			                           0      & \cdots           & 0      \\
			                           \vdots & \ddots           & \vdots \\
			                           0      & \cdots           & 0      \\
			                                  & e_{m1}\vect{r_1} &
		                           \end{bmatrix}+\cdots+
		\begin{bmatrix}
			0      & \cdots           & 0      \\
			\vdots & \ddots           & \vdots \\
			0      & \cdots           & 0      \\
			       & e_{mm}\vect{r_m} &
		\end{bmatrix}                                                                \\
		                 & =\begin{bmatrix}
			                    e_{11}\vect{r_1}+\cdots+e_{1m}\vect{r_m} \\
			                    e_{21}\vect{r_1}+\cdots+e_{2m}\vect{r_m} \\
			                    \vdots                                   \\
			                    e_{m1}\vect{r_1}+\cdots+e_{mm}\vect{r_m}
		                    \end{bmatrix}
	\end{align*}
\end{proof}

\begin{solution}
	Going back to the original problem, let's create an \textit{elementary matrix} to perform the first
	row operation from Solution \ref{sol:linear_sys_1} (assigning row $2$ to $\vect{r_2}-3\vect{r_1}$).
	Since this operation was done to remove the entry at the 2nd row and 1st column, we'll call this elementary matrix $\vect{E_{21}}$:
	\[\vect{E_{21}}\vect{A}=\begin{bmatrix}1&0&0\\-3&1&0\\0&0&1\end{bmatrix}\begin{bmatrix}1&2&1\\3&8&1\\0&4&1\end{bmatrix}=\begin{bmatrix}1&2&1\\0&2&-2\\0&4&1\end{bmatrix}\]
	Repeat the process:
	\[\vect{E_{32}}(\vect{E_{21}\vect{A}})=\begin{bmatrix}1&0&0\\0&1&0\\0&-2&1\end{bmatrix}\begin{bmatrix}1&2&1\\0&2&-2\\0&4&1\end{bmatrix}=\begin{bmatrix}1&2&1\\0&2&-2\\0&0&5\end{bmatrix}\]
\end{solution}
\end{document}
